# AUTO-GENERATED by experiment agent â€” validated via AST safety scan

from typing import Dict, Optional

import torch
import torch.nn.functional as F
from torch import nn


class CNNTransformerRegressor(nn.Module):
    """
    Hybrid CNN-Transformer for PPG heart rate regression.
    Uses CNN to extract local features, then Transformer for long-range dependencies.
    """

    def __init__(
        self,
        in_channels: int = 12,
        num_classes: int = 1,  # For regression
        embed_dim: int = 256,
        depth: int = 4,
        num_heads: int = 8,
        dropout: float = 0.1,
        primary_modality: str = "main",
    ) -> None:
        super().__init__()
        self.primary_modality = str(primary_modality)

        # CNN feature extractor
        self.conv_layers = nn.Sequential(
            nn.Conv1d(in_channels, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm1d(64),
            nn.ReLU(inplace=True),
            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),
            nn.BatchNorm1d(128),
            nn.ReLU(inplace=True),
            nn.Conv1d(128, embed_dim, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm1d(embed_dim),
            nn.ReLU(inplace=True),
        )

        # Positional encoding
        self.pos_encoding = nn.Parameter(torch.randn(1, 1000, embed_dim) * 0.02)

        # Transformer layers
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=embed_dim * 4,
            dropout=dropout,
            activation="relu",
            batch_first=True,
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)

        # Regression head
        self.head = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(embed_dim, embed_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(embed_dim // 2, num_classes),
        )

    def forward(
        self, signals: Dict[str, torch.Tensor], meta: Optional[dict] = None
    ) -> torch.Tensor:
        x = signals[self.primary_modality]  # (B, C, T)

        # CNN feature extraction
        x = self.conv_layers(x)  # (B, embed_dim, T')

        # Prepare for transformer: (B, embed_dim, T') -> (B, T', embed_dim)
        x = x.transpose(1, 2)

        # Add positional encoding
        seq_len = x.size(1)
        if seq_len <= self.pos_encoding.size(1):
            x = x + self.pos_encoding[:, :seq_len, :]
        else:
            # Interpolate positional encoding if sequence is longer
            pos_enc = F.interpolate(
                self.pos_encoding.transpose(1, 2), size=seq_len, mode="linear", align_corners=False
            ).transpose(1, 2)
            x = x + pos_enc

        # Transformer processing
        x = self.transformer(x)  # (B, T', embed_dim)

        # Back to (B, embed_dim, T') for pooling
        x = x.transpose(1, 2)

        # Regression head
        return self.head(x)  # (B, 1)
